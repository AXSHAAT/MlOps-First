

# ğŸš— Vehicle Data MLOps Project

A **production-grade MLOps project** demonstrating the full lifecycle of a machine learning system â€” from **data ingestion** to **model training, deployment, and CI/CD automation** using **AWS, Docker, MongoDB, and GitHub Actions**.

This project showcases my ability to design, implement, and deploy ML pipelines using **modern DevOps/MLOps best practices**.

---

## ğŸ“Œ Features

* âœ… **Automated Project Structure** with `template.py`
* âœ… **Local Package Management** via `setup.py` & `pyproject.toml`
* âœ… **Environment & Dependencies** managed using Conda + `requirements.txt`
* âœ… **MongoDB Atlas Integration** for dataset storage & retrieval
* âœ… **Custom Logging & Exception Handling**
* âœ… **End-to-End ML Pipeline**

  * Data Ingestion
  * Data Validation
  * Data Transformation
  * Model Training
  * Model Evaluation
  * Model Pushing to AWS S3
* âœ… **AWS Integration**

  * S3 for model storage
  * IAM for secure authentication
* âœ… **Prediction API with Flask (`app.py`)**
* âœ… **CI/CD with GitHub Actions**
* âœ… **Containerization & Deployment**

  * Dockerized application
  * AWS ECR for image registry
  * AWS EC2 for deployment
* âœ… **Scalable Inference Service** exposed via public endpoint

---

## âš™ï¸ Project Setup

### ğŸ”¹ Step 1: Project Initialization

```bash
python template.py
```

### ğŸ”¹ Step 2: Setup Local Packages

* `setup.py` and `pyproject.toml` configured for local imports.

### ğŸ”¹ Step 3: Virtual Environment & Dependencies

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
pip list
```

---

## ğŸ—„ï¸ MongoDB Setup

1. Create a **MongoDB Atlas project & cluster (M0 tier)**
2. Configure **username/password & network access (0.0.0.0/0)**
3. Obtain **connection string** for Python driver
4. Push dataset to MongoDB via Jupyter Notebook (`mongoDB_demo.ipynb`)
5. Validate data using MongoDB Atlas collections

---

## ğŸ“Š ML Pipeline Workflow

### ğŸ”¹ Data Handling

* **Data Ingestion**: Fetch & transform data from MongoDB â†’ DataFrame
* **Data Validation**: Schema-driven validation (`config/schema.yaml`)
* **Data Transformation**: Feature engineering + preprocessing

### ğŸ”¹ Model Lifecycle

* **Model Trainer**: Train ML model with pipeline architecture
* **Model Evaluation**: Compare new model vs. existing model with thresholding
* **Model Pusher**: Push best model â†’ AWS S3 bucket (`my-model-mlopsproj`)

---

## â˜ï¸ AWS Integration

* **IAM Users & Roles** for secure access
* **S3 Buckets** for model registry
* **Environment Variables** for credentials

---

## ğŸ³ CI/CD & Deployment

### ğŸ”¹ Dockerization

* `Dockerfile` & `.dockerignore` for container builds

### ğŸ”¹ GitHub Actions

* Workflow YAML for CI/CD
* AWS credentials stored as GitHub Secrets
* Push â†’ Build â†’ Deploy automatically

### ğŸ”¹ AWS EC2 Deployment

* **Ubuntu Server (24.04)**
* Self-hosted GitHub Runner
* Exposed app on **port 5080**

---

## ğŸš€ Usage

### Run Locally

```bash
python app.py
```

Access app at: `http://localhost:5080`

### Train Model

```bash
http://<public-ip>:5080/training
```

---

## ğŸ› ï¸ Tech Stack

* **Languages**: Python, YAML, Bash
* **ML**: Scikit-learn, Pandas, Numpy
* **DB**: MongoDB Atlas
* **Cloud**: AWS (S3, EC2, IAM, ECR)
* **MLOps Tools**: Docker, GitHub Actions, CI/CD Pipelines
* **Visualization**: Jupyter Notebook (EDA & Feature Engineering)

---

## ğŸ“Œ Project Architecture

```bash
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/        # Data ingestion, validation, transformation, trainer
â”‚   â”œâ”€â”€ configuration/     # MongoDB & AWS connections
â”‚   â”œâ”€â”€ entity/            # Config, Artifact, Estimator classes
â”‚   â”œâ”€â”€ pipeline/          # Training & Prediction pipelines
â”‚   â”œâ”€â”€ utils/             # Utilities (logging, exception handling, helpers)
â”‚
â”œâ”€â”€ notebook/              # Jupyter notebooks (EDA, MongoDB demo)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ app.py                 # Flask app entry point
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .github/workflows/     # GitHub Actions CI/CD configs
```

---

## ğŸŒŸ Highlights for Recruiters

* Implemented **full-stack MLOps pipeline** from scratch.
* Used **MongoDB Atlas + AWS S3** for hybrid data & model storage.
* Automated deployment via **GitHub Actions â†’ Docker â†’ AWS EC2**.
* Designed **modular & scalable architecture** for production ML systems.

---

ğŸ”¥ This project demonstrates not just ML model building but **end-to-end production deployment and automation** â€” the core of modern **MLOps practices**.

---

